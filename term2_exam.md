# Список тем к зачету за II семестр.

### Секция 1 - "классические" методы МО

- Задача бинарной классификации: вероятностная формулировка; байесовский классификатор (БК); способ обучения БК; решающее правило БК; свойства БК;
- Задача бинарной классификации: вероятностная формулировка, наивный байесовский классификатор (НБК); способ обучения НБК; решающее правило НБК;
- Задача бинарной классификации: вероятностная формулировка, модели линейного дискриминантного анализа (LDA) и квадратичного дискриминантного анализа (QDA): формулировка, способ обучения, решающее правило, разделяющая поверхность, основные свойства;
- Задача классификации: вероятностная формулировка; понятие разделяющей поверхности, уравнение разделяющей поверхности для произвольной вероятностной модели;
- Задача бинарной классификации: вероятностная формулировка; модель логистической регрессии: формулировка, функция потерь, способ обучения, разделяющая поверхность, решающее правило, основные свойства;
- Задача мультиномиальной классификации: вероятностная формулировка; модель мультиномиальной логистической регрессии: формулировка, функция потерь, способ обучения, разделяющая поверхность, решающее правило, основные свойства;
- Оценка качества моделей классификации: меры качества, базовые сценарии их применения;
- Обобщенные линейные модели (GLM): формулировка; линейная регрессия и логистическая регрессия (бинарная и мультиномиальная) как GLM; связь вида функции потерь с видом функции  связи GLM;
- Обобщенные аддитивные модели (GAM): формулировка; линейная регрессия и логистическая регрессия (бинарная и мультиномиальная) как GAM; связь вида функции потерь с видом функции  связи GAM и с видом преобразующих функций GAM;
- Метод опорных векторов (SVM) в задаче классификации: формулировка, схема работы, способ обучения; kernel trick; основные свойства SVM;
- Непараметрические методы МО. Деревья решений: формулировка, схема работы, функции ошибки, способ обучения, основные свойства;
- Ансамбли моделей: виды ансамблей, краткое описание подходов ансамблирования.
- Ансамбли моделей. Stacking: формулировка, метод обучения, способ применения, основные свойства;
- Ансамбли моделей. Bagging: формулировка, обоснование снижения дисперсии ответов, схема работы, способ обучения, основные свойства;
- Ансамбли моделей. Random Forests: применяемые подходы, формулировка, обоснование снижения дисперсии ответов, схема работы, способ обучения, основные свойства;
- Ансамбли моделей. Бустинг: формулировка, схема работы, способ обучения, основные свойства;
- Ансамбли моделей. Градиентный бустинг: формулировка, схема работы, способ обучения, основные свойства;



### Секция 2 - искусственные нейронные сети (ИНС)

- Многослойный перцептрон (MLP): формулировка, схема работы, функциональная запись, графическое представление; MLP как обобщенная линейная модель, MLP как обобщенная аддитивная модель; варианты функции потерь MLP в задачах регрессии и классификации;
- Многослойный перцептрон (MLP): формулировка, схема работы, функциональная запись, графическое представление; подсчет количества параметров MLP; способ обучения MLP



### Секция 3 - обучение ИНС (ИНС)

- Обучение ИНС: свойства ландшафта функции потерь современных ИНС; метод стохастического градиентного спуска (SGD), импульсный метод (momentum);
- Обучение ИНС: свойства ландшафта функции потерь современных ИНС; метод стохастического градиентного спуска (SGD), RMSProp;
- Обучение ИНС: граф вычислений ИНС, вычисление градиента функции потерь с помощью графа вычислений ИНС; алгоритм обратного распространения ошибки;
- Обучение ИНС: граф вычислений ИНС; вычисление градиента функции потерь с помощью графа вычислений ИНС; проблема затухающего/исчезающего градиента и ее решения; проблема взрывающего градиента и ее решения;
- Обучение ИНС: мотивация и способы специальной начальной инициализации ИНС;
- Обучение ИНС: пакетная нормализация (Batch Normalization): решаемая проблема и метод решения; альтернативный метод решения с помощью функций активации;
- Регуляризации ИНС: Решаемая проблема и методы решения: L1, L2; Dropout.