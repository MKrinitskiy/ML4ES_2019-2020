{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import autograd.numpy as np\n",
    "    from autograd import grad\n",
    "except:\n",
    "    print('Please make sure you installed autograd package!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as tnp\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mnorm\n",
    "from scipy.stats import kde\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_moons(2000, noise=0.1, shuffle=False)\n",
    "\n",
    "x2,y2 = make_moons(2000, noise=0.1, shuffle=False)\n",
    "x2 = x2[y2==0]\n",
    "y2 = y2[y2==0]\n",
    "y2 = y2+2\n",
    "x2 = x2*2.5\n",
    "x2[:,0] = x2[:,0]+0.5\n",
    "\n",
    "\n",
    "x = tnp.concatenate([x,x2], axis=0)\n",
    "\n",
    "y = tnp.concatenate([y,y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {0: 'red', 1: 'green', 2: 'blue', 3: 'gray'}\n",
    "cmaps = ['Reds', 'Greens', 'Blues', 'Greys']\n",
    "clabels = [colors[l] for l in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(3,3), dpi=300)\n",
    "plt.scatter(x[:,0], x[:,1], s=1, c=clabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(arr):\n",
    "    classes = np.unique(y)\n",
    "    num_classes = len(classes)\n",
    "    return classes,np.squeeze(np.eye(num_classes)[arr.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes,y_oh = one_hot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z_clipped = z-z.max()\n",
    "    log_softmax = z_clipped - np.log(np.sum(np.exp(z_clipped), axis=-1, keepdims=True))\n",
    "    softmax_values = np.exp(log_softmax)\n",
    "    return softmax_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    result = np.zeros_like(z)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # you need to implement sigmoid function of z\n",
    "    # the resulting matrix should be assigned to the \"result\" variable\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    result = np.zeros_like(z)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # you need to implement ReLU function of z\n",
    "    # the resulting matrix should be assigned to the \"result\" variable\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(X, theta):\n",
    "    p = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # you need to implement the neural network that computes the estimates of probabilities\n",
    "    # for each object of X for each class\n",
    "\n",
    "    # note that the size of p should be NxK\n",
    "    # where\n",
    "    # N is the number of objects in X\n",
    "    # K is the number of classes of the problem\n",
    "    \n",
    "    # (1) you need to get chunks of theta values out of theta of reasonable lengths\n",
    "    #     and reshape them into the correct theta matrices for each ANN layer\n",
    "    # (2) you need to implement layers themselves,\n",
    "    #     which includes matrix multiplication and the application of a non-linearity function\n",
    "    #     (sigmoid, ReLU of softmax)\n",
    "    #     make sure you did not forget biases for each layer\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_cross_entropy(p_pred, y_true):\n",
    "    return -np.sum(np.multiply(y_true, np.log(p_pred)), axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_cross_entropy_loss(X, y, theta, reg_alpha = 1.0e-3):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    p = compute_probabilities(X, theta)\n",
    "    l = multinomial_cross_entropy(p, y)\n",
    "    \n",
    "    l_reg = 0.0\n",
    "    # YOUR ADDITIONAL CODE HERE\n",
    "    # you need to implement the regularization L2 penalty term\n",
    "    # that helps to limit the values of parameters theta of the model\n",
    "    \n",
    "    return np.squeeze(np.mean(l, axis=0, keepdims=True))+l_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# you need to generate random theta values according to:\n",
    "#          - the number of features of input data X\n",
    "#          - the number of nodes in each layer, and the bias term\n",
    "#          - the number of output nodes (the number of classes K)\n",
    "# Then you need to flatten these matrices and and concatenate them into one vector\n",
    "# (due to the specifics of the function \"minimize\" of scipy package)\n",
    "#\n",
    "# assign the vector of theta values to the vatiable theta_start\n",
    "\n",
    "theta_start = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we are starting to optimize the model we have just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = multinomial_cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = grad(loss_fn, argnum=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loss = loss_fn(x,y_oh,theta_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loss_grad = grad_fn(x,y_oh,theta_start)\n",
    "curr_loss_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a callback that runs each iteration of the minimization loop\n",
    "# we are just logging the loss history here\n",
    "\n",
    "def minimization_callback(loss_history, curr_loss_value):\n",
    "    loss_history.append(curr_loss_value)\n",
    "    print('loss_value: %f' % curr_loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual minimization of the loss function of out model\n",
    "\n",
    "loss_history = []\n",
    "optimization_result = minimize(lambda t: float(loss_fn(x, y_oh, t)),\n",
    "                               theta_start,\n",
    "                               jac = lambda t: np.array(grad_fn(x, y_oh, t)).flatten(),\n",
    "                               callback = lambda t: minimization_callback(loss_history, float(loss_fn(x,y_oh,t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us plot the loss function evolution\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will get the theta values from the optimization result\n",
    "\n",
    "theta_result = optimization_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing probabilities using the optimized ANN weights and\n",
    "# its architecture described in the function compute_probabilities()\n",
    "\n",
    "pred_proba = compute_probabilities(x, theta_result)\n",
    "print(pred_proba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the probabilities into the class labels\n",
    "\n",
    "y_pred = classes[np.argmax(pred_proba, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the accuraacy score\n",
    "\n",
    "np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us plot the resulting probabilities for the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 400\n",
    "xmesh, ymesh = tnp.mgrid[-4:4:nbins*1j, -4:4:nbins*1j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_mesh = np.concatenate([xmesh.ravel()[:,np.newaxis], ymesh.ravel()[:,np.newaxis]], axis=-1)\n",
    "x_test_mesh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_mesh = compute_probabilities(x_test_mesh, theta_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mesh = classes[np.argmax(probas_mesh, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_mesh = probas_mesh.reshape(list(xmesh.shape) + [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mesh = preds_mesh.reshape(xmesh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(6,5), dpi=300)\n",
    "plt.scatter(x[:,0], x[:,1], s=1, c=clabels)\n",
    "for class_label,proba_mesh in zip([0,1,2], [probas_mesh[:,:,0], probas_mesh[:,:,1], probas_mesh[:,:,2]]):\n",
    "    pm = tnp.ma.array(proba_mesh)\n",
    "    pm.mask = (preds_mesh != class_label)\n",
    "    _ = plt.pcolormesh(xmesh, ymesh, pm, cmap=cmaps[class_label], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
